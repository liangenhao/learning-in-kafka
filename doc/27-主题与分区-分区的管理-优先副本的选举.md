---
title: 27-主题与分区-分区的管理-优先副本的选举
date: 2020-02-19
categories: learning-in-kafka
tags: [Kafka]
---

## 基本概念

- 【多副本机制】：

  - 分区使用<u>多副本机制</u>提升可靠性。

  - 只有<u>leader副本</u>对外提供读写服务；<u>follower副本</u>只负责在内部进行消息的同步。

  - 如果一个分区的leader副本不可用，那么这整个分区变得不可用。

    此时就需要从follower副本（ISR）中挑选一个新的leader副本。

  - **<u>从某种程度上，broker节点中leader副本个数的多少决定了这个broker节点负载的高低。</u>**

- 【关于节点负载】：

  - 主题创建时，主题的分区和副本会<u>尽可能均匀地分</u>布到各个broker节点上。
  - <u>leader副本</u>也会均匀的分配在不同的broker上。
  - 针对同一个分区而言，同一个broker节点中不可能出现它的多个副本，即<u>一个broker中最多只能有它的一个副本。</u>
  - 当某个leader副本故障时，就会选举一个follower副本作为leader。此时就会导致集群的节点负载不均衡。

## 优先副本-preferred replica

- 【目的】：为了有效的<u>治理负载失衡</u>的情况，引入优先副本概念。

- 【定义】：在AR集合列表中的第一个副本。

  - <u>理想状态下，优先副本就是该分区的leader副本。</u> - preferred leader
  - <u>kafka确保主题的优先副本在集群中均匀分布。</u>

- 如果leader分布过于集中，就会造成集群负载不均衡。

  > 当分区的leader副本不再是优先副本时，容易造成不均衡。

### 优先副本选举（分区平衡）

- 【定义】：通过一定的方式促使优先副本选举为leader副本。这个动作称为"分区平衡"。

- 【目的】：保证分区均衡。

- 【注意】：分区平衡不意味着集群的负载均衡。

  > 集群的负载均衡需要考虑：集群中的分区分配是否均衡；每个分区的leader副本的负载是否均衡；等等...

#### 分区自动平衡

- 【前提】：

  - broker端参数：`auto.leader.rebalance.enable`设置为true。（默认开启）

- 【定时任务】：

  - Kafka控制器会启动一个定时任务，轮询所有的broker节点，计算每个broker节点的<u>分区不平衡率</u> 是否超过`leader.imbalance.per.broker.percentage`参数配置的值（默认10%）。
  - 如果超过设置的比值则会自动执行优先副本的选举动作以求分区平衡。
  - 执行周期由`leader.imbalance.check.interval.seconds`控制（默认300秒，即5分钟）

  > 分区不平衡率 = 非优先副本的leader个数 / 分区总数

- 【注意】：

  - 不建议将`auto.leader.rebalance.enable`设置为true。因为可能会引起负面的性能问题。

- 【不使用分区自动平衡的其他解决方法】：

  - 不管：一定程度上的不平衡也是可以忍受的。
  - 告警：增加埋点指标设置相应的告警，然后执行手动分区平衡。
  - 手动分区平衡。

#### 分区手动平衡（推荐）

- 【前提】：
  
- broker端参数：`auto.leader.rebalance.enable`设置为false。
  
- 【所有分区进行优先副本选举】：

  - 执行脚本：

    ```shell
    bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka_cluster
    ```

  - 示例：

    - 首先创建一个分区数为3，副本因子为3的主题`topic-partitons`

      ```shell
      bin/kafka-topics.sh --zookeeper localhost:2181/kafka_cluster --create --topic topic-partitions --partitions 3 --replication-factor 3
      ```

    - 查看该主题详情：

      ```shell
      bin/kafka-topics.sh --zookeeper localhost:2181/kafka_cluster --describe --topic topic-partitions
      
      Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs:
      	Topic: topic-partitions	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
      	Topic: topic-partitions	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
      	Topic: topic-partitions	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1
      ```

      此时三个分区的leader副本是均匀分布的。分别在三个节点上

    - 将brokerId为2的节点重启

      ```shell
      bin/kafka-topics.sh --zookeeper localhost:2181/kafka_cluster --describe --topic topic-partitions
      
      Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs:
      	Topic: topic-partitions	Partition: 0	Leader: 1	Replicas: 0,1,2	Isr: 1,0,2
      	Topic: topic-partitions	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,0,2
      	Topic: topic-partitions	Partition: 2	Leader: 1	Replicas: 2,0,1	Isr: 1,0,2
      ```

      此时，三个分区的leader副本都在brokerId为1的节点上。

    - 执行优先副本选举脚本，再查看主题详情：

      ```shell
      # 优先副本选举
      bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka_cluster
      
      Created preferred replica election path with topic-config-1,topic-admin-1,topic-create-zk-2,topic-create-0,topic-partitions-1,topic-admin-0,topic-create-zk-3,topic-config-0,topic-admin-4,topic-config-2,topic-create-api-0,topic-partitions-2,topic-create-zk-0,topic-admin-3,topic-admin-2,topic-partitions-0,topic-create-1,topic-create-2,topic-create-zk-1,topic-create-3
      Successfully started preferred replica election for partitions Set(topic-config-1, topic-admin-1, topic-create-zk-2, topic-create-0, topic-partitions-1, topic-admin-0, topic-create-zk-3, topic-config-0, topic-admin-4, topic-config-2, topic-create-api-0, topic-partitions-2, topic-create-zk-0, topic-admin-3, topic-admin-2, topic-partitions-0, topic-create-1, topic-create-2, topic-create-zk-1, topic-create-3)
      
      # 查看
      bin/kafka-topics.sh --zookeeper localhost:2181/kafka_cluster --describe --topic topic-partitions
      
      Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs:
      	Topic: topic-partitions	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 1,0,2
      	Topic: topic-partitions	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,0,2
      	Topic: topic-partitions	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 1,0,2
      ```

      此时三个分区的leader副本又是均匀分配的了。

  - 注意：

    - 在优先副本选举过程中，具体的元数据信息会被存入zookeeper的`/admin/preferred_replica_election`节点中，如果这些数据超过了zookeeper节点允许的大小，就会选举失败，默认下zookeeper允许的节点大小为1MB。

- 【部分分区进行优先副本选举】（推荐）：

  - 执行脚本：

    ```shell
    bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka_cluster --path-to-json-file <文件路径>
    ```

    > 通过`path-to-json-file`来小批量的<u>对部分分区执行优先副本的选举操作</u>。
    >
    > 文件示例：
    >
    > ```json
    > {
    >     "partitions":[
    >         {
    >             "partition":0,
    >             "topic":"topic-partitions"
    >         },
    >         {
    >             "partition":1,
    >             "topic":"topic-partitions"
    >         },
    >         {
    >             "partition":2,
    >             "topic":"topic-partitions"
    >         }
    >     ]
    > }
    > ```

  - 示例：

    ```shell
    bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka_cluster --path-to-json-file ../path-to-json-file/election.json
    
    Created preferred replica election path with topic-partitions-0,topic-partitions-1,topic-partitions-2
    Successfully started preferred replica election for partitions Set(topic-partitions-0, topic-partitions-1, topic-partitions-2)
    ```